{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd2e80b6-c58a-4a1f-9917-274ba5aeefcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f50889e-461d-4b55-b22a-f3436d737016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpt4all in c:\\users\\chandini\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\chandini\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gpt4all) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chandini\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gpt4all) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chandini\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chandini\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chandini\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chandini\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\chandini\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->gpt4all) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt4all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b4b975-f088-455a-87d6-85a3981b7c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████| 1.98G/1.98G [04:58<00:00, 6.63MiB/s]\n",
      "Verifying: 100%|██████████████████████████████████████████████████████████████████| 1.98G/1.98G [00:09<00:00, 215MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,8\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "output = model.generate(\"The capital of France is \", max_tokens=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382b8f41-acfa-4172-bcc2-50cab875e1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 euros\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\"The capital of France is \", max_tokens=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9e62b90-2794-44bf-b53c-c7dcab445e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': '### System:\\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\\n\\n'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': ' Hello! How can I assist you today?'}, {'role': 'user', 'content': 'write me a short poem'}, {'role': 'assistant', 'content': \" Sure, here's a short poem for you: \\n\\nThe sun sets in the sky so high, \\nAs the stars come out to light the way. \\nThe world grows quiet, still and calm, \\nAs we sit and watch the night go by. \\n\\nThe moon rises, casting shadows near, \\nAs the crickets chirp their lullaby. \\nAnd in that moment, we feel alive, \\nIn this peaceful, tranquil scene so true.\"}, {'role': 'user', 'content': 'thank you'}, {'role': 'assistant', 'content': \" You're welcome! I'm glad to help.\"}]\n"
     ]
    }
   ],
   "source": [
    "with model.chat_session():\n",
    "    response1 = model.generate(prompt='hello', temp=0)\n",
    "    response2 = model.generate(prompt='write me a short poem', temp=0)\n",
    "    response3 = model.generate(prompt='thank you', temp=0)\n",
    "    print(model.current_chat_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16063a04-4d5e-4904-9de9-a81f95530360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdc5dd82-88ca-4c85-9c9a-84138dfc806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': '### System:\\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\\n\\n'}, {'role': 'user', 'content': ' I would like to create synthetic data based on 6 emotions. Can you create 10 outputs based on the emotion sadness, first person, and each less than 30 words?'}, {'role': 'assistant', 'content': \" Sure! Here are ten outputs based on the emotion of sadness, with first-person pronouns:\\n\\n1. I feel sad because I lost my favorite toy.\\n2. I am feeling sad because I didn't get that promotion at work.\\n3. I am feeling very sad right now because my dog passed away.\\n4. I am feeling sad about my friend who just got engaged.\\n5. I am feeling really down today because I don't have anyone to talk to.\\n6. I am feeling sad because I can't seem to find the motivation to get up and go to work.\\n7. I am feeling very sad right now because I lost my phone charger.\\n8. I am feeling sad about my grandma who passed away last year.\\n9. I am feeling really down today because I don't like the weather outside.\\n10. I feel very sad because I can't seem to find the\"}]\n"
     ]
    }
   ],
   "source": [
    "with model.chat_session():\n",
    "    response1 = model.generate(prompt=' I would like to create synthetic data based on 6 emotions. Can you create 10 outputs based on the emotion sadness, first person, and each less than 30 words?', temp=0)\n",
    "    # response2 = model.generate(prompt='write me a short poem', temp=0)\n",
    "    # response3 = model.generate(prompt='thank you', temp=0)\n",
    "    print(model.current_chat_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45867f10-74d2-4263-90fe-b616664be826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████| 4.21G/4.21G [03:35<00:00, 19.5MiB/s]\n",
      "Verifying: 100%|██████████████████████████████████████████████████████████████████| 4.21G/4.21G [00:21<00:00, 198MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "model2 = GPT4All(\"gpt4all-falcon-newbpe-q4_0.gguf\")\n",
    "output = model.generate(\"The capital of France is \", max_tokens=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d79a9ab3-4ebf-42bb-82b5-0dd0f107ba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████| 4.11G/4.11G [03:40<00:00, 18.7MiB/s]\n",
      "Verifying: 100%|██████████████████████████████████████████████████████████████████| 4.11G/4.11G [00:20<00:00, 200MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model3 = GPT4All(\"mistral-7b-openorca.gguf2.Q4_0.gguf\")\n",
    "output = model.generate(\"The capital of France is \", max_tokens=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "613cd65d-78be-4d1a-8e59-77ebe4c796b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████| 7.37G/7.37G [06:07<00:00, 20.0MiB/s]\n",
      "Verifying: 100%|██████████████████████████████████████████████████████████████████| 7.37G/7.37G [00:35<00:00, 208MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6\n"
     ]
    }
   ],
   "source": [
    "model4 = GPT4All(\"wizardlm-13b-v1.2.Q4_0.gguf\")\n",
    "output = model.generate(\"The capital of France is \", max_tokens=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae3d406-e1de-46f9-bb05-0e9cc761c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████| 7.37G/7.37G [22:56<00:00, 5.35MiB/s]\n",
      "Verifying: 100%|██████████████████████████████████████████████████████████████████| 7.37G/7.37G [00:35<00:00, 210MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.\n"
     ]
    }
   ],
   "source": [
    "model5 = GPT4All(\"orca-2-13b.Q4_0.gguf\")\n",
    "output = model.generate(\"The capital of France is \", max_tokens=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a364f-d13f-4dc2-8e7a-4570c9256f0d",
   "metadata": {},
   "source": [
    "###generate(prompt, *, max_tokens=200, temp=0.7, top_k=40, top_p=0.4, min_p=0.0, repeat_penalty=1.18, repeat_last_n=64, n_batch=8, n_predict=None, streaming=False, callback=_pyllmodel.empty_response_callback)\r\n",
    "Generate outputs from any GPT4All model.\r\n",
    "\r\n",
    "Parameters:\r\n",
    "\r\n",
    "prompt (str) – The prompt for the model the complete.\r\n",
    "max_tokens (int, default: 200 ) – The maximum number of tokens to generate.\r\n",
    "temp (float, default: 0.7 ) – The model temperature. Larger values increase creativity but decrease factuality.\r\n",
    "top_k (int, default: 40 ) – Randomly sample from the top_k most likely tokens at each generation step. Set this to 1 for greedy decoding.\r\n",
    "top_p (float, default: 0.4 ) – Randomly sample at each generation step from the top most likely tokens whose probabilities add up to top_p.\r\n",
    "min_p (float, default: 0.0 ) – Randomly sample at each generation step from the top most likely tokens whose probabilities are at least min_p.\r\n",
    "repeat_penalty (float, default: 1.18 ) – Penalize the model for repetition. Higher values result in less repetition.\r\n",
    "repeat_last_n (int, default: 64 ) – How far in the models generation history to apply the repeat penalty.\r\n",
    "n_batch (int, default: 8 ) – Number of prompt tokens processed in parallel. Larger values decrease latency but increase resource requirements.\r\n",
    "n_predict (int | None, default: None ) – Equivalent to max_tokens, exists for backwards compatibility.\r\n",
    "streaming (bool, default: False ) – If True, this method will instead return a generator that yields tokens as the model generates them.\r\n",
    "callback (ResponseCallbackType, default: empty_response_callback ) – A function with arguments token_id:int and response:str, which receives the tokens from the model as they are generated and stops the generation by ###returning False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3b7bdcc-4e0c-4fa7-bb5c-feb03422b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "with model2.chat_session():\n",
    "    prompt_msg = 'I would like to create synthetic data based on 6 emotions. Can you create 10 outputs based on the emotion sadness, first person, and each less than 30 words?'\n",
    "    # response1 = model.generate(prompt=, temp=0)\n",
    "    tokens = []\n",
    "    for token in model.generate(prompt_msg, streaming=True):\n",
    "        tokens.append(token)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68035c43-bdf7-4f17-8e8b-2d2dc7f8f0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# model3 = GPT4All(\"mistral-7b-openorca.gguf2.Q4_0.gguf\")\n",
    "\n",
    "system_template = 'A chat between a curious user and an artificial intelligence assistant.\\n'\n",
    "# many models use triple hash '###' for keywords, Vicunas are simpler:\n",
    "prompt_template = 'USER: {0}\\nASSISTANT: '\n",
    "with model3.chat_session(system_template, prompt_template):\n",
    "    prompt_msg = 'I would like to create synthetic data based on 6 emotions. Can you create 10 outputs based on the emotion sadness, first person, and each less than 30 words?'\n",
    "\n",
    "    response1 = model.generate(prompt_msg)\n",
    "    print(response1)\n",
    "    # print()\n",
    "    # response2 = model.generate('why is the sky blue?')\n",
    "    # print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5959c08-7655-4b8d-91c0-62272f64bebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- I feel so sad today.\n",
      "- It's not easy being alone.\n",
      "- I miss my friends and family.\n",
      "- I can't stop crying.\n",
      "- I don't know how to deal with this feeling.\n",
      "\n",
      "\n",
      "The color of the sky appears to change throughout the day, with the sun's position and angle on the horizon affecting its appearance. During sunset, the sky takes on a reddish hue due to the scattering of sunlight by the Earth's atmosphere. As the night wears on, the sky becomes darker and more indigo in color due to the absorption of longer wavelengths of light. The blue color of the sky is caused by the scattering of shorter wavelengths of light by the Earth's atmosphere.\n"
     ]
    }
   ],
   "source": [
    "# from gpt4all import GPT4All\n",
    "# model4 = GPT4All('wizardlm-13b-v1.2.Q4_0.gguf')\n",
    "system_template = 'A chat between a curious user and an artificial intelligence assistant.\\n'\n",
    "# many models use triple hash '###' for keywords, Vicunas are simpler:\n",
    "prompt_template = 'USER: {0}\\nASSISTANT: '\n",
    "with model4.chat_session(system_template, prompt_template):\n",
    "    # prompt_msg = 'I would like to create synthetic data based on 6 emotions. '\n",
    "\n",
    "    response1 = model.generate('Create 10 outputs based on the emotion sadness, first person, and each less than 30 words?')\n",
    "    print(response1)\n",
    "    print()\n",
    "    response2 = model.generate('why is the sky blue?')\n",
    "    print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5c7ff7a-6568-4b03-881c-73c6fc4126a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm sorry to hear that. I can help you with that.\n",
      "\n",
      "I'm sorry to hear that. Let me know if there is anything I can do to help you feel better.\n",
      "\n",
      "1. I feel sad today.\n",
      "2. It's not easy being alone.\n",
      "3. I miss my friends.\n",
      "4. I can't stop crying.\n",
      "5. I don't want to let go of this feeling.\n",
      "6. I feel so heavy with sadness.\n",
      "7. I wish someone could understand what I'm going through.\n",
      "8. I feel like a burden on those around me.\n",
      "9. I wish I could turn back time and undo this pain.\n",
      "10. I can't escape the darkness that surrounds me.\n",
      "\n",
      "I'm sorry to hear that. I can help you with that. Can you please provide me with some more information about what you need help with so I can assist you better?\n",
      "\n",
      "1. I feel sad today.\n",
      "2. It's not easy being sad all by yourself.\n",
      "3. I miss my friends when I'm feeling sad.\n",
      "4. Sometimes it feels like the world is against you when you're feeling sad.\n",
      "5. I try to keep busy and distract myself from feeling sad.\n",
      "6. When I feel sad, I try to be kind to myself and take care of my needs.\n",
      "7. It's important to remember that sadness can be a normal part of life.\n",
      "8. Sometimes it helps to talk about how you're feeling when you're feeling sad.\n",
      "9. It's okay to feel sad, but I try not to let it control me.\n",
      "10. When I'm feeling sad, I focus on the things that make me happy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from gpt4all import GPT4All\n",
    "# model4 = GPT4All('wizardlm-13b-v1.2.Q4_0.gguf')\n",
    "system_template = 'A chat between a curious user and an artificial intelligence assistant.\\n'\n",
    "# many models use triple hash '###' for keywords, Vicunas are simpler:\n",
    "prompt_template = 'USER: {0}\\nASSISTANT: '\n",
    "with model4.chat_session(system_template, prompt_template):\n",
    "    # prompt_msg = 'I would like to create synthetic data based on 6 emotions. '\n",
    "\n",
    "    for i in range(5):\n",
    "        response1 = model.generate('Create 10 outputs based on the emotion sadness, first person, and each less than 30 words?')\n",
    "        print(response1)\n",
    "    print()\n",
    "    # response2 = model.generate('why is the sky blue?')\n",
    "    # print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c70ed4c-88ae-427c-bb40-688aa58c0b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9162e14f-127e-451e-93ad-6f5b3cd86395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model5 = GPT4All(\"orca-2-13b.Q4_0.gguf\")\n",
    "\n",
    "with model5.chat_session():\n",
    "    response1 = model.generate(prompt=' I would like to create synthetic data based on 6 emotions. Can you create 10 outputs based on the emotion sadness, first person, and each less than 30 words?', temp=0)\n",
    "    # response2 = model.generate(prompt='write me a short poem', temp=0)\n",
    "    # response3 = model.generate(prompt='thank you', temp=0)\n",
    "    print(model.current_chat_session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
